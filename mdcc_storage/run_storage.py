# example usage
# python3 run_exp.py CONFIG=exp_profiles/test.json NODE_ID=0 DEBUG_MODE=debug [
# optional args]
# the first argument is the path to your exp settings, note that for any params with a list as 
# the values meaning multiple exps will be issued under each value
# the last argument is the index of current node corresponding to the ifconfig.txt
import os, sys, re, os.path
import subprocess, json, paramiko
import threading

ifconfig = "ifconfig.txt"


class myThread(threading.Thread):

    def __init__(self, conn, cmd, exit_on_err=False, print_stdout=True):
        threading.Thread.__init__(self)
        self.conn = conn
        self.cmd = cmd
        self.exit_on_err = exit_on_err
        self.print_stdout = print_stdout

    def run(self):
        print("[run_exp.py] executing remotely: " + self.cmd)
        stdin, stdout, stderr = self.conn[1].exec_command(self.cmd)
        if stderr.read() == b'':
            if not self.print_stdout:
                return 0
            for line in stdout.readlines():
                print("[remote-{}] ".format(self.conn[0]) + line.strip())
        else:
            print("[run_exp.py] error executing: {}".format(self.cmd))
            print(stderr.read())
            if self.exit_on_err:
                exit(0)
            return 1
        return 0


def load_environment(fname="info.txt"):
    env = {
        "home": os.path.expanduser('~') + "/"
    }
    path_to_file = env["home"] + "Sundial-Private/" + fname
    if os.path.exists(path_to_file):
        lines = [line.strip() for line in open(path_to_file)]
        env["user"] = lines[0]
        env["repo"] = lines[1]
    else:
        f = open(path_to_file)
        env["user"] = input("[run_exp.py] enter user name: ")
        f.write(env["user"] + "\n")
        env["repo"] = os.getcwd()
        if input("[run_exp.py] confirm home directory : {}, y/n? ".format(
                env["repo"])) != "y":
            env["repo"] = input("[run_exp.py] enter home directory: ")
        if env["repo"][-1] != "/":
            env["repo"] = env["repo"] + "/"
        f.write(env["repo"] + "\n")
    env["working"] = env["repo"] + "mdcc_storage/"
    return env


# system methods
def exec(cmd, exit_on_err=False):
    print("[run_exp.py] executing: " + cmd)
    try:
        subprocess.run(cmd, shell=True, check=True)
    except Exception as e:
        print("[run_exp.py] error executing: {}".format(cmd))
        print(e)
        if exit_on_err:
            exit(0)
        return 1
    return 0


def remote_exec(conn, cmd, exit_on_err=False, print_stdout=True,
                skip_warning=False):
    print("[run_exp.py] executing remotely: " + cmd)
    stdin, stdout, stderr = conn[1].exec_command(cmd)
    err = stderr.read().decode("utf-8")
    if len(err) > 0 and "This file was generated by a newer version of protoc" \
            not in err:
        warning_only = True
        print("[run_exp.py] error executing: {}".format(cmd))
        print("stderr: [remote-{}] \"".format(conn[0]) + err.strip() + "\"")
        if "error" in err.lower():
            warning_only = False
        if exit_on_err:
            if warning_only and skip_warning:
                return 0
            exit(0)
        return 1
    else:
        if not print_stdout:
            return 0
        for line in stdout.readlines():
            print("stdout: [remote-{}] ".format(conn[0]) + line.strip())
    return 0


# job loading methods
def load_job(args):
    # generate a dictionary based on a string
    # update experiment profile with specified string format configurations
    def parse_arg(arg):
        job = {}
        for item in arg:
            key = item.split("=")[0]
            value = item.split("=")[1]
            job[key] = value
        return job

    job_tmp = parse_arg(args)
    if "CONFIG" in job_tmp:
        print("[run_exp.py] loading config from {} ...".format(job_tmp["CONFIG"]))
        job = json.load(open(job_tmp["CONFIG"]))
        job.update(job_tmp)
    else:
        job = job_tmp
    return job


def compress_job(job):
    arg = ""
    for key in job:
        if key == "CONFIG":
            continue
        arg += "{}={} ".format(key, job[key])
    return arg


def load_ipaddr(curr_node, env):
    # EXCLUDED CURRENT NODE
    nodes = {}
    f = open(ifconfig)
    itr = 0
    for addr in f:
        if addr[0] == '#':
            continue
        elif itr == curr_node:
            itr += 1
            continue
        elif addr[0] == '=' and addr[1] == 'l':
            break
        elif itr == env["num_nodes"]:
            break
        print(
            "[run_exp.py] try to connect to: node {} at {}".format(itr, addr.split(":")[0]))
        con = paramiko.SSHClient()
        con.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        con.load_system_host_keys()
        con.connect(addr.split(":")[0], username=env["user"],
                    key_filename="{}.ssh/id_ed25519".format(env["home"]))
        nodes[itr] = (addr.split(":")[0], (itr, con))
        itr += 1
    f.close()
    return nodes


# execution methods
def build_config(env, job):
    def replace(filename, pattern, replacement):
        f = open(filename)
        s = f.read()
        f.close()
        s = re.sub(pattern, replacement, s)
        f = open(filename, 'w')
        f.write(s)
        f.close()

    # build configuration file locally
    exec("cp {}{} {}{}".format(env["working"], "config-std.h", env["working"],
                               "config.h"))
    for (param, value) in job.items():
        pattern = r"\#define\s*" + re.escape(param) + r'.*'
        if "ADDR" in param:
            replacement = "#define " + param + ' \"' + str(value) + '\"'
        else:
            replacement = "#define " + param + ' ' + str(value)
        replace("{}config.h".format(env["working"]), pattern, replacement)


def kill_nodes(user, nodes):
    print("[run_exp.py]  Failed. KILLING CURRENT SERVER ... ")
    for itr in nodes:
        remote_exec(nodes[itr][1], "sudo pkill rundb")
        print("[run_exp.py]  kill node {}".format(itr))


def parse_output(env, job, fname):
    output = open(fname)
    phase = 0
    success = False
    for line in output:
        if phase == 0:
            if "=Worker Thread=" in line:
                phase = 1
                success = True
                continue
        elif phase == 1:
            if "=Input/Output Thread=" in line:
                phase = 2
                continue
            line = line.strip()
            if ":" in line:
                # for token in line.strip().split('[summary]')[-1].split(','):
                list = line.split(':')
                # list = re.split(r'\s+|:\s+', line)
                key = list[0].strip()
                list[1] = list[1].strip()
                val = re.split(r'\s+', list[1])[0]
                job[key] = val
        # break
    output.close()
    if success:
        os.system("rm -f " + fname)
        stats = open("{}outputs/stats.json".format(env["repo"]), 'a+')
        stats.write(json.dumps(job) + "\n")
        stats.close()
    return success


def log_to_errors(job, fname):
    # log to errors
    if "EXP_ID" in job:
        i = job["EXP_ID"]
    else:
        i = 0
    if "CONFIG" in job:
        exp_name = job["CONFIG"].split('/')[-1].split('.')[0]
    else:
        exp_name = "unnamed"
    logpath = "../log/"
    logfile = "error_{}.list".format(exp_name)
    os.makedirs(logpath, exist_ok=True)
    error_log = open(logpath + logfile, "a+")
    error_log.write("{}, {}\n".format(i, job))
    error_log.close()
    os.system("cp {} {}error_{}_{}.out".format(fname, logpath, exp_name, i))


def start_nodes(env, job, nodes, compile_only=True):
    # build configuration
    build_config(env, job)

    # try compile locally
    os.chdir("{}".format(env["working"]))
    exec("sudo pkill rundb;")
    exec("make clean > {}temp.out 2>&1".format(env["working"]),
         exit_on_err=True)
    exec("./compile.sh > {}temp.out 2>&1".format(env["repo"], env["working"]),
         exit_on_err=True)
    exec("rm -f {}temp.out".format(env["working"]))

    # compile remotely
    for itr in nodes:
        # copy config file
        exec("scp -r config.h {}@{}:{}config.h".format(
            env["user"], nodes[itr][0], env["working"]), exit_on_err=True)
        # compile
        remote_exec(nodes[itr][1], "sudo pkill rundb; ")
        remote_exec(nodes[itr][1], "{}compile.sh".format(
            env["working"]), exit_on_err=True, print_stdout=False,
                    skip_warning=True)
    if compile_only:
        return

    # execute
    threads = []
    for itr in nodes:
        print("[run_exp.py]  starting node {}".format(itr))
        # start server remotely
        # use another thread to do it asynchronously
        full_cmd = """./run.sh -Gn{} | tee {}outputs/temp.out""".format(
            itr, env["repo"])
        thread = myThread(nodes[itr][1], full_cmd)
        thread.start()
        threads.append(thread)

    # start server locally
    os.chdir(env["wokring"])
    ret = exec("""./run.sh -Gn{} | tee {}outputs/temp-{}.out""".format(
        env["curr_node"], env["repo"], env["curr_node"]))
    if ret != 0:
        kill_nodes(env["user"], nodes)
        exit(0)

    # wait for completion
    for t in threads:
        t.join()

    # process results
    # copy temp from every non-failed node and rename it
    for itr in nodes:
        addr = nodes[itr][0]
        # copy config file
        exec("scp {}@{}:{}outputs/temp.out {}outputs/temp-{}.out".format(
            env["user"], addr, env["repo"], env["repo"], itr))
    # then execute process command for each one.
    for itr in nodes:
        job["NODE_ID"] = itr
        # if not successfully parsing, write to log
        if not parse_output(env, job, "{}outputs/temp-{}.out".format(
                env["repo"], itr)):
            log_to_errors(job, "{}outputs/temp-{}.out".format(env["repo"], itr))
    # process self results
    if not parse_output(env, job, "{}outputs/temp-{}.out".format(
            env["repo"], env["curr_node"])):
        log_to_errors(job, "{}outputs/temp-{}.out".format(env["repo"],
                                                          env["curr_node"]))


def test(env, nodes, job):
    if env["num_nodes"] > 1:
        exec("python3 install.py sync {} {}".format(
            env["curr_node"], "0-{}".format(env["num_nodes"]-1)),
            exit_on_err=True)
    mode = job.get("MODE", "compile")
    if mode == "release" or mode == "debug":
        start_nodes(env, job, nodes, compile_only=False)
    elif mode == "compile":
        start_nodes(env, job, nodes, compile_only=True)


if __name__ == "__main__":
    env = load_environment()
    # load job and start exp
    job = load_job(sys.argv[1:])
    # get current node and num_nodes
    env["curr_node"] = int(job.get("NODE_ID", "0"))
    num_nodes = job.get("NUM_NODES", 2)
    if isinstance(num_nodes, list):
        env["num_nodes"] = max([int(i) for i in num_nodes])
    else:
        env["num_nodes"] = int(num_nodes)
    # establish connections to all remote nodes
    nodes = load_ipaddr(env["curr_node"], env)
    test(env, nodes, job)
